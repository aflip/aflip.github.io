[
  {
    "objectID": "blog/biostat-with-python-01/01-BWP-deploy-bold.html",
    "href": "blog/biostat-with-python-01/01-BWP-deploy-bold.html",
    "title": "Statistical tests for biomedical research with python",
    "section": "",
    "text": "This resource was created for Medibuddy\nDr. Arpitha Jacob helped with corrections and improvements"
  },
  {
    "objectID": "blog/biostat-with-python-01/01-BWP-deploy-bold.html#requirements",
    "href": "blog/biostat-with-python-01/01-BWP-deploy-bold.html#requirements",
    "title": "Statistical tests for biomedical research with python",
    "section": "Requirements",
    "text": "Requirements\n\n#  Dev conditions:\n#  Python 3.11.7\n#  Pandas 2.2.0\n#  numpy 1.26.4\n#  Pingouin 0.5.4\n#  scipy 1.12.0\n#  seaborn 0.13.2\n#  matplotlib 3.8.2\n\n\n# Libraries needed to run this notebook\n\n# !pip install pandas pingouin scipy seaborn matplotlib\n\n\n# Files needed to use the default examples in this notebook\n\n# !wget https://raw.githubusercontent.com/aflip/biostats-with-python/main/data/age-bmi-sbp-data.csv\n# !wget https://raw.githubusercontent.com/aflip/biostats-with-python/main/data/paired-t-test-data.csv\n# !wget https://raw.githubusercontent.com/aflip/biostats-with-python/main/data/mann-whitney-eva-data.csv\n# !wget https://raw.githubusercontent.com/aflip/biostats-with-python/main/data/paired-t-bp.csv"
  },
  {
    "objectID": "blog/biostat-with-python-01/01-BWP-deploy-bold.html#tests-by-variable-type",
    "href": "blog/biostat-with-python-01/01-BWP-deploy-bold.html#tests-by-variable-type",
    "title": "Statistical tests for biomedical research with python",
    "section": "Tests by variable type",
    "text": "Tests by variable type\n\nCategorical vs categorical\n\nChi Squared\n\nCategorical vs continuous\n\nWilcoxon-Mann-Whitney (Independent groups)\nWilcoxon Signed Rank (Related or paired groups)\nIndependent t-test\nPaired t-test\nAncova\nAnova\n\nContinuous vs continuous\n\nPearson’s R\nSpearman’s Rank\nWilcoxon Signed Rank\nStudent’s t-test"
  },
  {
    "objectID": "blog/biostat-with-python-01/01-BWP-deploy-bold.html#non-parametric",
    "href": "blog/biostat-with-python-01/01-BWP-deploy-bold.html#non-parametric",
    "title": "Statistical tests for biomedical research with python",
    "section": "Non parametric",
    "text": "Non parametric\nIn the context of biomedical research and biostoatistics, Non parametric tests are those that do not make or need some assumptions about the population or distributions involved. Eg. The assumption of normality. At large sample sizes, the power of a non parametric test is the same as that of a parametric test. At smaller sample sizes, they allow us to consider hypothesis that the requirements for parametric tests would not allow. These also allow us to test categorical data.\nAn alternative way to define non parametric tests is that the exact sampling distribution (t, chi etc.) can be obtained without requiring knowledge of the joint probability distribution of X and Y.\n\nPearson\nType: Parametric\nApplied on: Continuous (interval or ratio) independent variable and continuous dependent variable.\nTalk: In this test we determine the strength of linear association between the outcome variable (x) and independent predictor variable (y). The test assumes a linear relationship.\nRange: -1 &lt;= r &lt;= 1\nA positive r indicates a positive relationship (x increases as y increases) and if the two are not correlated the r would be 0.\nr = \\frac{\\Sigma(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\Sigma(x_i - \\bar{x})^2\\Sigma(y_i - \\bar{y})^2}}\nThe formula is covariance divided by the product of standard deviations, so it is essentially a normalized measurement of the covariance. As with covariance itself, the measure can only reflect a linear correlation of variables, and ignores and misfires for other types of relationships or correlations. This is important because if the relationship is non linear you may still get get a strong r but is meaningless.\nSignificance values and confidence intervals can be calculated from an r using various methods.\nAlways do a scatter plot, but do not conclude purely visually\nNull Hypothesis: h_0: There is no correlation between the two\nAssumptions:\n\nThe variables are related linearly\n\nNon linear relationships can make interpreting r difficult. Here are some visual examples. from wikipedia\n\nInterpretation\nThe interpretation depends on the scientific field. Because different standards apply. The original Coehn’s recommendations were\n\nr ≈ 0.10: small effect.\nr ≈ 0.30: moderate effect.\nr ≈ 0.50: large effect.\n\nAlternately:\n\n\n\nr\nStrength of correlation\n\n\n\n\n0.0 &gt; 0.1\nno correlation\n\n\n0.1 &gt; 0.3\nweak correlation\n\n\n0.3 &gt; 0.5\nmedium correlation\n\n\n0.5 &gt; 0.7\nhigh correlation\n\n\n0.7 &gt; 1\nvery high correlation\n\n\n\nDepending on the type of study, other tests like bootstrapped estimations, Fisher’s z calculation etc. might be needed\nLimitations:\n\nCorrelation depends on the range of the data so that larger ranges lead to larger correlations. This can lead to vary strange interpretations\n\nExtra:\nCorrelattion =/= Agreement. When measuring the correlation between two observations on the same phenomenon using different methods, it is important to differentiate correlation from agreement. eg. QC scores between different agents or spirometry vs peak flow meter. Since pearson’s is commonly used to compare different tests, it is good to be aware of this and measure them using other methods.\nRead more:\n\nBBR: Correlations\nMeta Analysis in R: Effect Sizes\n\nDataset:\nHidaka T, Kasuga H, Endo S, Masuishi Y, Kakamu T, Takeda A, et al. Are lifestyle pattern changes associated to poor subjective sleep quality?: a cross-sectional study by gender among the general Japanese population underwent specified medical checkups in 2014 and 2015. Zenodo; 2020.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"age-bmi-sbp-data.csv\")\n\nThis dataset gives us blood pressures, fasting blood glucose hba1c, smoking status and other information about a sample population. This is derived from the dataset linked above, and only contains values taken at baseline, in 2014. What we will do here is to investigate the baseline correlations.\n\ndf.head()\n\n\n\n\n\n\n\n\nid\ngender\n2014Age\n2014BMI\n2014SBP\n2014DBP\n2014FPG\n2014HbA1c\n2014smoking\n\n\n\n\n0\n3\n1\n64\n24.2\n170\n90\n138.0\n6.0\n2\n\n\n1\n9\n1\n72\n21.4\n110\n64\n95.0\n5.4\n2\n\n\n2\n14\n1\n63\n23.5\n116\n70\n94.0\n5.3\n1\n\n\n3\n16\n1\n66\n24.3\n105\n74\n103.0\n5.4\n2\n\n\n4\n18\n1\n72\n25.7\n130\n70\n101.0\n5.7\n2\n\n\n\n\n\n\n\nWhen the dataset is small and well structured, this is a good way to get an overall picture.\n\nimport seaborn as sns\n\nsns.set_palette(\"Set2\")\n\ng = sns.PairGrid(df[[\"2014BMI\", \"2014SBP\", \"2014DBP\", \"2014FPG\", \"2014HbA1c\"]])\ng.map_diag(sns.histplot, kde=True)  # sets what will be the diagonal\ng.map_offdiag(sns.scatterplot)\n\n\n\n\n\n\n\n\nVisually, the 2014HbA1c seems correlated with the 2014FPG, the distribution of BMI appears normal. What else?\nLet’s see if the blood pressure and BMI are correlated\n\nimport scipy.stats as stats\n\nstats.pearsonr(x=df[\"2014SBP\"], y=df[\"2014BMI\"])\n\nPearsonRResult(statistic=0.2075587788303957, pvalue=0.0)\n\n\nA correlation of 0.2 is seen, which means there is a weak positive correlation between the two. In Python, if the p value is very small, the p value shows up as 0.0.\nFor practical purposes this can be reported as p &lt;0.0000001\nExercise: Which variables do you think would be positively correlated? Draw histograms and do correlations to see if true.\nPandas allows us to run correlations on the entire dataset and with seaborn we could plot the whole thing as a heatmap\n\ndf.corr()\n\n\n\n\n\n\n\n\nid\ngender\n2014Age\n2014BMI\n2014SBP\n2014DBP\n2014FPG\n2014HbA1c\n2014smoking\n\n\n\n\nid\n1.000000\n-0.107238\n-0.140646\n0.066077\n0.025006\n0.056320\n0.093893\n-0.037453\n-0.045123\n\n\ngender\n-0.107238\n1.000000\n-0.026324\n-0.144869\n-0.110618\n-0.158681\n-0.189004\n-0.038417\n0.265688\n\n\n2014Age\n-0.140646\n-0.026324\n1.000000\n-0.011325\n0.182881\n-0.016351\n0.093073\n0.123198\n0.124332\n\n\n2014BMI\n0.066077\n-0.144869\n-0.011325\n1.000000\n0.207559\n0.202590\n0.210351\n0.198723\n0.023227\n\n\n2014SBP\n0.025006\n-0.110618\n0.182881\n0.207559\n1.000000\n0.648800\n0.163331\n0.076390\n0.021739\n\n\n2014DBP\n0.056320\n-0.158681\n-0.016351\n0.202590\n0.648800\n1.000000\n0.085319\n0.005988\n-0.002997\n\n\n2014FPG\n0.093893\n-0.189004\n0.093073\n0.210351\n0.163331\n0.085319\n1.000000\n0.723343\n-0.022481\n\n\n2014HbA1c\n-0.037453\n-0.038417\n0.123198\n0.198723\n0.076390\n0.005988\n0.723343\n1.000000\n0.005497\n\n\n2014smoking\n-0.045123\n0.265688\n0.124332\n0.023227\n0.021739\n-0.002997\n-0.022481\n0.005497\n1.000000\n\n\n\n\n\n\n\n\nsns.heatmap(df.corr(), annot=True)\n\n\n\n\n\n\n\n\nA strong positive correlation between FPG and hba1c and sbp and dbp are seen. Everything else has lower correlations, but there is a positive correlation between BMI and HbA1c as we expect.\n\n\nSpearman’s rank correlation\nType: Non parametric ranking correlation test\nApplied on: Ordinal independent variable and an ordinal dependent variable.\nTalk:\nSpearman’s \\rho (sometimes labeled r_s) assumes a monotonic relationship between X and Y , not a linear (as opposed to pearson’s r)\nA monotonic relationship is where\n\nWhen X increases, Y always increases, or always decreases or stays flat.\nThe rate at which an increase or decrease occurs does not have to be the same for both variables.\nThe relationship could be linear or non linear.\n\n$h_0: $ There is no correlation between the two variables \\rho=0\nFrom this a t statistic can be calculated, and the p values can be computed.\nt = \\frac{\\rho\\sqrt{n-2}}{\\sqrt{1-\\rho^{2}}}\nRead more:\n\nBBR:Spearman’s Correlation\nStack Exchange: Spearman’s and Kendall Tau\nWikipedia\n\n\n# First, modelling with a random generated dataset\n\nimport random\n\nage = []\nfor i in range(10000):\n    age.append(random.randint(18, 100))\n\nsmoking = []\nfor i in range(10000):\n    smoking.append(random.randint(0, 4))\n\n# smoking ranks :\n# 0= Never smoked,\n# 1 = used to smoke,\n# 2 = smoke less than a pack a day ,\n# 3 = smoking a pack a day,\n# 4 = smoking more than a pack a day\n\n\nimport pandas as pd\n\ndf_random = pd.DataFrame({\"age\": age, \"smoking\": smoking})\n\n\nimport scipy.stats as stats\n\nstats.normaltest(df_random[\"age\"])  # not a normal dist\n\nNormaltestResult(statistic=8448.011390155185, pvalue=0.0)\n\n\n\ndata1 = df_random[\"age\"]\ndata2 = df_random[\"smoking\"]\n\n\ncoef, p = stats.spearmanr(data1, data2)\n\ncoef, p\n\n(-0.00621671431731578, 0.5342051023681593)\n\n\nSpearmans correlation coefficient: -0.01309\np=0.1905 &gt; alpha\nSamples are uncorrelated (fail to reject h_0)\n\n# Real World dataset\n\nimport pandas as pd\nimport scipy.stats as stats\n\ndf = pd.read_csv(\"age-bmi-sbp-data.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nid\ngender\n2014Age\n2014BMI\n2014SBP\n2014DBP\n2014FPG\n2014HbA1c\n2014smoking\n\n\n\n\n0\n3\n1\n64\n24.2\n170\n90\n138.0\n6.0\n2\n\n\n1\n9\n1\n72\n21.4\n110\n64\n95.0\n5.4\n2\n\n\n2\n14\n1\n63\n23.5\n116\n70\n94.0\n5.3\n1\n\n\n3\n16\n1\n66\n24.3\n105\n74\n103.0\n5.4\n2\n\n\n4\n18\n1\n72\n25.7\n130\n70\n101.0\n5.7\n2\n\n\n\n\n\n\n\n\ndata1 = df[\"2014Age\"][:5000]\ndata2 = df[\"2014smoking\"][:5000]\n\n\ncoef, p = stats.spearmanr(data1, data2)\ncoef, p\n\n(0.13033529753985648, 2.1769251455759564e-20)\n\n\nSpearmans correlation coefficient: -0.130\np = 2.1769251455759564e-20 &lt;&lt; alpha\nSamples are correlated (reject h_0)\n\n\nChi-Squared\nType: Non-parametric\nApplied to: Categorical independent variable and a categorical dependent variable\nTalk:\nSometimes the variables you need to test for independence are both categorical. This test looks at if two or more categorical variables are related\nEg. You gave subjects mushrooms and measured if they got high, or not. The test was done for two groups, one has had previous exposure to psychedelic drugs and the other did not. Now if you want to know if people got high irrespective of the previous exposure, this test comes in handy.\nBut the test does not give us a way to verify a directional hypothesis. That is, chi-square value can not tell you what the relationship is between two variables, only that a relationship exists between the two variables. There is only a right tail on a chi-square distribution.\nThe test computes the expected frequencies assuming that the two variables were not correlated, and then compares this with the observed frequencies in your sample. Correlation is run on this to get the chi statistic.\nNull Hypothesis:\nh_0: There is no relationship between x and y. \\space\\chi^2 = 0\nAssumptions:\n\nThe total number of samples is greater than 13, otherwise exact tests (such as Barnard’s Exact test) should be used\nThere are at least 5 observations expected per category (with low n, \\space\\chi^2 will miss existing relationships)\nThe sum of the observed ( o_e) and expected frequencies ( f_e) must be the same\n\nLimitations:\n\nHeavily influenced by n. As n increases, chi-square increases, independent of the relationship between x and y. At large numbers you will likely find statistically significant relationships when in reality there is no relationship (False Positives). With small numbers might not find statistically significant relationships even if they exist (False Negatives).\nSensitive to small expected frequencies. If f_e&lt;5 in 1 or more cells, chi-square is unreliable.\n\nRead More: Robert Lowry Chi-Square Procedures for the Analysis of Categorical Frequency Data\nExtra:\nThis test can be used to\n\nEvaluate the quality of a categorical variable in a classification problem\nCheck the similarity between two categorical variables.\n\n\nA good categorical predictor and the class column should present high \\space\\chi^2 and low p-value.\nDissimilar categorical variables should present high \\space\\chi^2 and low p-value.\n\nFrom Pingouin Docs\nIn scipy, the path to getting the relevent info is a bit circuitous, but can be accessed via scipy.stats.chisquare, Here I am using Pingouin instead\n\nimport pingouin as pg\n\ndata = pg.read_dataset(\"chi2_independence\")\ndata.head()\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n1\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n1\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n1\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n1\n\n\n\n\n\n\n\n\ndata.target.value_counts()  # in this dataset, `target` is the presense and absence of cardiovascular disease, expressed as a category.\n\ntarget\n1    165\n0    138\nName: count, dtype: int64\n\n\n\ndata.sex.value_counts()\n\nsex\n1    207\n0     96\nName: count, dtype: int64\n\n\nLet’s say our hypothesis is that gender and the target are interacting or have some correlation.\n\nexpected, observed, stats = pg.chi2_independence(data, x=\"sex\", y=\"target\")\n\nIf gender and heart disease status were un correlated we would see these frequencies.\nRobert Lowry Chi-Square Procedures Provides the method this is calculated in detail.\n\nexpected  \n\n\n\n\n\n\n\ntarget\n0\n1\n\n\nsex\n\n\n\n\n\n\n0\n43.722772\n52.277228\n\n\n1\n94.277228\n112.722772\n\n\n\n\n\n\n\n\nobserved  # more male cases seem to be having the target than expected\n\n\n\n\n\n\n\ntarget\n0\n1\n\n\nsex\n\n\n\n\n\n\n0\n24.5\n71.5\n\n\n1\n113.5\n93.5\n\n\n\n\n\n\n\n\n43.722 + 52.277 + 94.277228 + 112.722772  # sum exp\n\n302.999\n\n\n\n24.5 + 71.5 + 113.5 + 93.5  # sum obs\n\n303.0\n\n\n\nstats  # the p values are all very low, with great power (comes great responsibility).\n\n\n\n\n\n\n\n\ntest\nlambda\nchi2\ndof\npval\ncramer\npower\n\n\n\n\n0\npearson\n1.000000\n22.717227\n1.0\n1.876778e-06\n0.273814\n0.997494\n\n\n1\ncressie-read\n0.666667\n22.931427\n1.0\n1.678845e-06\n0.275102\n0.997663\n\n\n2\nlog-likelihood\n0.000000\n23.557374\n1.0\n1.212439e-06\n0.278832\n0.998096\n\n\n3\nfreeman-tukey\n-0.500000\n24.219622\n1.0\n8.595211e-07\n0.282724\n0.998469\n\n\n4\nmod-log-likelihood\n-1.000000\n25.071078\n1.0\n5.525544e-07\n0.287651\n0.998845\n\n\n5\nneyman\n-2.000000\n27.457956\n1.0\n1.605471e-07\n0.301032\n0.999481\n\n\n\n\n\n\n\nGiven that p &lt; alpha, we reject the null hypothesis that gender and heart disease are uncorrelated. Gender is a good predictor.\n\\chi^2 tests also have critical values, which can be looked up in a table online. But if the p vaues are all &lt; alpha \\chi^2 will be more than the critical values. So we can reject the h_0 that there is no relationship between the variables\n\n\nWilcoxon–Mann–Whitney (Wilcoxon rank-sum test)\nType: Non parametric\nApplied on: Two categorical (ordinal/nominal) independent variables and a continuous dependant variable.\nTalk: Let’s say you have two conditions under which some observations were made (or two groups of people), and you would like to know if the observations suggest a difference of some kind between the groups. If the observations were normally distributed, an independent t-test could be used. But in case they were not, we use a MWU.\nThe test reports the probability that a randomly selected value from one group would be larger than a randomly selected one from the second group. This is best reported as concordance probability.\nNull hypothesis:\nh_0 : The distribution underlying sample x is the same as the distribution underlying sample y.\nAssumptions:\n\nAll the observations from both groups are independent of each other,\nThe responses are ordinal or ratio\n\nRead More:\n\nstats.stackexchange: How to choose between t-test or non-parametric\nBBR: Wilcox Mann Whitney\n\nThe dataset used below is from table 15.1 Discovering Statistics Using R by Andy Field, Jeremy Miles, Zoë Field\nIn this dataset, a psychiatrist studies the effect of some drugs on mood.\n\nThey study 20 participants\n10 were given an ecstasy tablet to take on a Saturday night (day 0) and\n10 were allowed to drink only alcohol.\nLevels of depression were measured for both groups using the Beck Depression Inventory (BDI) on day 1 and day 4.\n\nWe would like to know if there is any statistically difference in the BDI scores on day 1 and day 3, split across the two groups. eg. Did Ecstacy make mood worse or better?\nour h_0 is that there is no difference between the two groups on day 3 or day 1\n\nimport pandas as pd\n\ndf = pd.read_csv(\n    \"mann-whitney-eva-data.csv\",\n)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nParticipant\nDrug\nBDI_Sunday\nBDI_Wednesday\n\n\n\n\n0\n1\nEcstasy\n15\n28\n\n\n1\n2\nEcstasy\n35\n35\n\n\n2\n3\nEcstasy\n16\n35\n\n\n3\n4\nEcstasy\n18\n24\n\n\n4\n5\nEcstasy\n19\n39\n\n\n\n\n\n\n\n\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\nstats.probplot(\n    df[\"BDI_Sunday\"], dist=\"norm\", plot=plt\n);  # the colon skips printing the data\n\n# this is ~ a q-q plot and there are outliers\n\n# Todo why this plot and what's a better plot\n\n\n\n\n\n\n\n\n\nx_s = df[df[\"Drug\"] == \"Ecstasy\"][\"BDI_Sunday\"]\ny_s = df[df[\"Drug\"] == \"Alcohol\"][\"BDI_Sunday\"]\n\nstats.mannwhitneyu(x_s, y_s)\n\n# the differences on BDI sunday are not significant.\n\nMannwhitneyuResult(statistic=64.5, pvalue=0.28608212723292936)\n\n\n\nimport pingouin as pg\n\npg.mwu(x_s, y_s)\n\n# P value &gt; Alpha. Accept the null hypothesis, the distributions are identical on day 1\n\n\n\n\n\n\n\n\nU-val\nalternative\np-val\nRBC\nCLES\n\n\n\n\nMWU\n64.5\ntwo-sided\n0.286082\n-0.29\n0.645\n\n\n\n\n\n\n\n\nx_w = df[df[\"Drug\"] == \"Ecstasy\"][\"BDI_Wednesday\"]\ny_w = df[df[\"Drug\"] == \"Alcohol\"][\"BDI_Wednesday\"]\n\nstats.mannwhitneyu(x_w, y_w)\n\nMannwhitneyuResult(statistic=96.0, pvalue=0.0005690342182204761)\n\n\np-value &lt; alpha, reject the null hypothesis, the sample differs on day 4.\nIn addition to the U statistic, it is useful and better to report the concordance probability (also known as probability index)\nc = U_1 /(n_1 * n_2)\n96.0/(10*10) = 0.96\nEffect size:\nIf you use scipy, effect size needs to be calculated manually, if you use pingouin, the RBC (ranked biserial correlation) and the common language effect size (CLES) will give you this information.\nManually calculating the effect size.\nfrom z score: r = z/sqrt(N)\nfrom U statistic : r=  1- (2U/n_1*n_2)\n\n\nWilcoxon signed-rank test\nType: Non-parametric\nApplied on: Continuous dependent variable and categorical independent variable\nLet’s say you have two measurements taken from the same individual, object, or related units and the data are not normal. If your data were normal, you could use the paired t-test to compare them. IN cases where the sample is not normal, we use the WSR test. It tests whether the distribution of the differences x - y is symmetric about zero.\neg.\n\nA measurement taken at two different times (e.g., pre-test and post-test score with an intervention administered between the two time points)\nA measurement taken under two different conditions (e.g., completing a test under a “control” condition and an “experimental” condition)\nMeasurements taken from two halves or sides of a subject or experimental unit (e.g., measuring hearing loss in a subject’s left and right ears).\n\nThe procedure of the test subtracts x from y and assigns signed ranks (-1, -2, +9 etc). If the measurements are from the same individual with no effect, you expect the ranks to be more or less the same.\nA z value can be calculated from this by z = \\frac{\\sum{SR_{i}}}{\\sqrt{\\sum{SR_{i}^{2}}}}\nand a p value can be looked up in a normal distribution stats.norm.cdf(z)\nNull Hypothesis: h_0: There is no difference in the ranks, the two paired samples come from the same distribution\nAssumptions:\n\nRandom sampling from a defined population\nInterval or ratio scale of measurement (continuous data)\nIndependent variable is two categorical, “related groups” or “matched pairs”\n\nRead More: 1. BBR: One sample test 2. Boston University: Non Parametric tests\n\ndf = pd.read_csv(\"paired-t-test-data.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nBDI_Sunday\nBDI_Wednesday\n\n\n\n\n0\n28\n29\n\n\n1\n19\n28\n\n\n2\n30\n33\n\n\n3\n21\n25\n\n\n4\n19\n32\n\n\n\n\n\n\n\nIn scipy When y is provided, wilcoxon calculates the test statistic based on the ranks of the absolute values of d = x - y. Roundoff error in the subtraction can result in elements of d being assigned different ranks even when they would be tied with exact arithmetic. Rather than passing x and y separately, always pass the difference x - y, rounding as needed to ensure that only truly unique elements are numerically distinct, and passing the result as x with no y.\n\ndf[\"BDI_diff\"] = df.BDI_Sunday - df.BDI_Wednesday\ndf.head()\n\n\n\n\n\n\n\n\nBDI_Sunday\nBDI_Wednesday\nBDI_diff\n\n\n\n\n0\n28\n29\n-1\n\n\n1\n19\n28\n-9\n\n\n2\n30\n33\n-3\n\n\n3\n21\n25\n-4\n\n\n4\n19\n32\n-13\n\n\n\n\n\n\n\n\nimport scipy.stats as stats\n\nstats.wilcoxon(df[\"BDI_diff\"])\n\n# p &lt; alpha, reject the null hypothesis that the two are from same distribution\n\nWilcoxonResult(statistic=348.0, pvalue=7.110924467959148e-13)\n\n\n\nfrom math import sqrt\n\nz = df.BDI_diff.sum() / sqrt(sum(df.BDI_diff * df.BDI_diff))\nz\n\n-7.045084942753576\n\n\n\np = 2 * (1 - stats.norm.cdf(abs(z)))\np\n\n1.8534063173092363e-12"
  },
  {
    "objectID": "blog/biostat-with-python-01/01-BWP-deploy-bold.html#parametric",
    "href": "blog/biostat-with-python-01/01-BWP-deploy-bold.html#parametric",
    "title": "Statistical tests for biomedical research with python",
    "section": "Parametric",
    "text": "Parametric\n\nIndependent T-test\nType: Parametric\nApplied to: Two independent categorical variables and one continuous dependant variable.\nTalk:\nThis test is used when there are two experimental conditions and different participants were assigned to each condition, but the outcome variable is the same. Let’s say we test the effect of a drug that reduced heart rates. One group you gave this to is powerlifters, the other group is badminton player. You measure the baseline heart rate and the difference from baseline at hour 2. So does this drug lower heart rate in powerlifters more than badminton players? A statistical measure of this is the mean value. And so we are looking at the quantity of interest or QOI = \\mu_{1}-\\mu_{2}\nThe calculation is t = \\frac{\\mathrm{point~estimate~of~QOI}}{\\mathrm{se~of~numerator}}\nNull hypothesis:\nH_{0}: \\mu_{1}=\\mu_{2}\nThe two samples are from the same population / the drug has no effect / there is no mean diff in the populations\nAssumptons:\n\nThe sampling distribution is normally distributed.\nData are interval or ratio\nScores in different treatment conditions are independent (because they come from different people)\nHomogeneity of variance – in theory we assume equal variances, but in reality we don’t\n\nRead More:\n\nBBR: Two sample T-test\n\n\nimport numpy as np\n\n# as a test, I am generating a random distribution here.\n\n\nmean_1, sd_1 = 120, 1  # mean and standard deviation\nμ, σ = 85, 1  # standard notation\n\n\ns = np.random.default_rng().normal(mean_1, sd_1, 10000)\nn = np.random.default_rng().normal(μ, σ, 10000)\n\n\n# To manually test if the data are normal\n# abs(mean_1 - np.mean(s))\n# abs(sd_1 - np.std(s, ddof=1))\n\n\nimport seaborn as sns\n\nsns.histplot(s)\nsns.histplot(n)\n\n\n\n\n\n\n\n\n\nstats.probplot(n, dist=\"norm\", plot=plt);\n\n# The data is pretty uniformly distributed\n\n\n\n\n\n\n\n\n\nimport scipy.stats as stats\n\nres = stats.ttest_ind(s, n)\nres\n\nTtestResult(statistic=2486.1432755104966, pvalue=0.0, df=19998.0)\n\n\n\nres.confidence_interval(confidence_level=0.95)\n\nConfidenceInterval(low=34.98009656277974, high=35.03529690340728)\n\n\nFor interpreting the result, we look at the p value, if it is less than the alpha, we reject the null hypothesis.\nIf you want to calculate the effect size, use earlier methods, or use pg, it gives the cohen-d\ngiven that we used a random number generator to ensure that these are infact independent, the result is not a surprise.\n\n# power = 1- (2*mean)/n1*n2\n\nr = 1 - ((2 * -138.48151) / (1000 * 1000))\nr  # this will be the same as power\n\n1.00027696302\n\n\n\nimport pingouin as pg\n\npg.ttest(s, n)\n\n/home/darthfader/miniforge3/envs/MS-baby-311/lib/python3.11/site-packages/pingouin/bayesian.py:152: RuntimeWarning: divide by zero encountered in scalar divide\n  bf10 = 1 / ((1 + t**2 / df) ** (-(df + 1) / 2) / integr)\n\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n2486.143276\n19998\ntwo-sided\n0.0\n[34.98, 35.04]\n35.159375\ninf\n1.0\n\n\n\n\n\n\n\n\n\nPaired T-test\nParametric.\nApplied to: Continuous Dependent variable and categorical independent variable\nWe use a paired t-test when we have two measurements taken from the same individual, object, or related units under different conditions. A paired t-test checks if the mean change for these pairs is significantly different from zero. Let’s say we come up with a pill that makes working memory better for 10 minutes. Instead of cases and controls, we can estimate the relative effect of the pill on working memory by first testing memory at base line, and then administering the drug and retesting. Same test, done on the same individual, before and after an intervention.\neg.\n\nA measurement taken at two different times (e.g., pre-test and post-test score with an intervention administered between the two time points)\nA measurement taken under two different conditions (e.g., completing a test under a “control” condition and an “experimental” condition)\nMeasurements taken from two halves or sides of a subject or experimental unit (e.g., measuring hearing loss in a subject’s left and right ears).\n\nAlso known as a paired sample t-test or a dependent samples t test. These names reflect the fact that the two samples are paired or dependent because they are of the same subjects.\nProcedure: we first subtract the pre-treatment value from the post-treatment value for each subject, this gives us the differences in the values. Then we compare these differences to zero.\nt\\ =\\ \\cfrac{\\mu{_d}\\ -\\ 0}{\\hat{\\sigma}/\\sqrt{n}}\nwhere \\mu{_d} is the sample mean of the differences, and \\sigma is the standard deviation of these differences.\nRemember that \\sigma/\\sqrt{n} gives us the standard error of the mean\nNull Hypothesis:\nh_0:\\mu{_d}=0\nAssumptions:\n\nRandom sampling from a defined population\nInterval or ratio scale of measurement (continuous data)\nPopulation is normally distributed\n\nRead More: 1. Directory of statistical Analysis 2. Kent State University: Paired T test\n\ndf = pd.read_csv(\"paired_t_bp.csv\")\n\nIn this dataset, we gave a drug to people and checked baseline mean BP and after 3 hours.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 2 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   base_bp      10000 non-null  float64\n 1   post_drg_bp  10000 non-null  float64\ndtypes: float64(2)\nmemory usage: 156.4 KB\n\n\n\ndf.base_bp.hist()\n\n\n\n\n\n\n\n\n\n# test if the data are normal\n\nimport scipy.stats as stats\n\nprint(stats.normaltest(df[\"base_bp\"]))\nprint(stats.normaltest(df[\"post_drg_bp\"]))\n\n# p &gt; alpha, data are  normal\n\nNormaltestResult(statistic=1.4720610341243716, pvalue=0.47901157493967916)\nNormaltestResult(statistic=1.0681931715578785, pvalue=0.5861986311814571)\n\n\n\nprint(\n    \"base Mean:\",\n    round(df[\"base_bp\"].mean()),\n    \"post drug Mean:\",\n    round(df[\"post_drg_bp\"].mean()),\n    \"Expected Mean Difference:\",\n    0,\n    \"Observed Mean Difference:\",\n    (df[\"base_bp\"].mean() - df[\"post_drg_bp\"].mean()),\n)\n\nbase Mean: 120 post drug Mean: 110 Expected Mean Difference: 0 Observed Mean Difference: 9.865290000000002\n\n\n\nimport scipy.stats as stats\n\nstats.ttest_rel(df[\"base_bp\"], df[\"post_drg_bp\"])\n\nTtestResult(statistic=85.77186477218346, pvalue=0.0, df=9999)\n\n\np &lt; alpha, reject null hypothesis that there is no mean difference. Whichever drug was used, it changes the mean bp\nThe results are statistically significant. Our sample data support the notion that the average paired difference does not equal zero. Specifically, the post drug mean is lower than the basal mean.\n\nimport pingouin as pg\n\npg.ttest(df[\"base_bp\"], df[\"post_drg_bp\"], paired=True)\n\n/home/darthfader/miniforge3/envs/MS-baby-311/lib/python3.11/site-packages/pingouin/bayesian.py:152: RuntimeWarning: divide by zero encountered in scalar divide\n  bf10 = 1 / ((1 + t**2 / df) ** (-(df + 1) / 2) / integr)\n\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n85.771865\n9999\ntwo-sided\n0.0\n[9.64, 10.09]\n1.22507\ninf\n1.0\n\n\n\n\n\n\n\nbf10 uses a bayesian method to calculate the stat and can be ignored. the p is too small, this causes the error\n\n\nStudent’s t-Test or 1-sample T-test\nType: Parametric\nApplied to: Continuous dependant variable, compared to the mean for the population (IV).\nTalk: If you would like to know if the sample observations are from the X population whose mean is known. Or, is there a difference between this group and the population.\neg. We have measured the heights of 30 people in the office, and want to know if the average height in the office \\hat x, that matches the average heights of indians \\mu_0.\nt_{calc} = \\frac{\\bar{x} - \\mu_{0}}{se_{\\hat x}}\nwhere {se_{\\hat x}} is the standard error of our observations {\\hat x}\nAfter you get a t value, you need to look up the critical value from a table, for the corresponding degree of freedom, and if $t_{calc} &gt; t_{crit}$, we can reject the null hypothesis.\nOne thing to remember in this test is that rejecting the null hypothesis is often the “bad” outcome in the student’s T\nNull Hypothesis:\nThe h_0 is that \\hat x = \\mu_0\nAssumptions:\n\nRandom sampling from a defined population\nInterval or ratio scale of measurement\nPopulation is normally distributed\n\nRead More:\n\nBBR: One sample test for Mean\nStatistics How to: 1 sample t-test\n\n\nimport numpy as np\n\nrng = np.random.default_rng()\n\n# Making a random dataset of 300 observations\n\nheight_1 = rng.integers(140, 180, 300)  # samples integers uniformly\n\n\nnp.mean(height_1)\n\n160.35\n\n\n\nfrom scipy import stats\n\n\nPOP_MEAN = 158.5  # from average heights of indians, wikipedia\n\n# perform one sample t-test\n\nstats.ttest_1samp(a=height_1, popmean=POP_MEAN)\n\nTtestResult(statistic=2.74954487960015, pvalue=0.0063311279678636674, df=299)\n\n\np &gt; alpha, accept the null hypothesis\nmeaning our sample mean is from the same population as the general Indian population\n\npg.ttest(height_1, POP_MEAN)  # Gives us the power and the Cohen-D\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n2.749545\n299\ntwo-sided\n0.006331\n[159.03, 161.67]\n0.158745\n2.612\n0.782523\n\n\n\n\n\n\n\nWe redid the test at the Amsterdam office, and want to know if that has the same mean as the indian office.\n\nheight_2 = rng.integers(168, 195, 300)\n\npd.DataFrame(height_2).hist(bins=5);\n\n\n\n\n\n\n\n\n\nstats.ttest_1samp(a=height_2, popmean=POP_MEAN)\n\nTtestResult(statistic=48.735063956440214, pvalue=2.747487708524111e-144, df=299)\n\n\np &lt; alpha. Reject the null hypothesis, our observed sample mean is not from the same population as India and our amsterdam office needs more diversity. Or taller tables.\n\n\nAnova\nType: Parametric\nApplied to:\n\nDV = continuous\nIV = categorical with more than 2 categories, usually 3-6 categories, nominal or ordinal (It gets very confusing when there are more than 6)\n\nRationale:\nIn some studies we may want to look at how different groups of patients or subjects differ with regards to the dependant variable or the outcome. If there are just two such groups, we can use the student’s t-test. But if multiple groups are present, we use Anova.\nThe groups may be in the same categorical variable, or in different categorical variables.\nEg. The pain tolerance of people with 0, 3, 6 or 10 tattoos was measured. Now we want to know if besides the number of tattoos, the gender made any difference. In the first case we have one categorical variable, presense of tattoos, with four groups. IN the second case we have four groups in category one, and two groups in category two.\nNull hypothesis:\nh_0: There is no effect of the IV on the DV\nAssumptions for doing an anova\n\nThe groups should be independent (comparison is to be between groups, not within subjects)\nThe data should be normally distributed, if small sample (&lt;30 observations per group), in larger samples this doesn’t matter so much. Normality should be checked for each category of the IV\nThe groups should have equal variance (homoscedasticity). Check using box plots /stripplots or Levene Test and do a welch anova if unequal variance.\nThere should not be any major outliers\n\nNote:\nWhen the ANOVA is significant, post hoc tests are used to see differences between specific groups. Post hoc tests control the family-wise error rate (inflated type I error rate) due to multiple comparisons. They adust the p values (Bonferroni correction) or critical value (Tukey’s HSD test).\nRead More: 1. Jammie Price : Anova 2. Biostatistics for research: Two Way anova 3. Handbook of Biological statistics: One Way Anova 4. Wikipedia: Multiple comparisons 5. Chen, S. Y et al. A general introduction to adjustment for multiple comparisons. Journal of thoracic disease, 9(6), 1725–1729\n\nimport pandas as pd\nimport pingouin as pg\n\ndf = pg.read_dataset(\"anova\")  # built-in dataset in pingouin\n\n# In this dataset they tested people's pain threshold and tried to see if the hair color of the subject made any difference to the thresholds.\n\n\ndf.head()\n\n\n\n\n\n\n\n\nSubject\nHair color\nPain threshold\n\n\n\n\n0\n1\nLight Blond\n62\n\n\n1\n2\nLight Blond\n60\n\n\n2\n3\nLight Blond\n71\n\n\n3\n4\nLight Blond\n55\n\n\n4\n5\nLight Blond\n48\n\n\n\n\n\n\n\n\ndf.rename(\n    columns={\"Hair color\": \"hair_color\", \"Pain threshold\": \"pain_threshold\"},\n    inplace=True,\n)\n\n# Just to make it easier to index into\n\nBox plots are everywhere and some care must be taken in interpreting them: Read more\n\nimport seaborn as sns\n\nsns.boxplot(data=df, x=\"hair_color\", y=\"pain_threshold\")\n\n\n\n\n\n\n\n\nVisualizing the data helps us get a rough idea of the distribution. in this case the values are distributed evenly, but box plots confuse me so I recommend using strip plots or violin plots\n\nimport matplotlib.pyplot as plt\n\n\"\"\" \n    This first plots a strip plot, then draws a line at the mean of each group, by plotting a box plot on top of the strip plot but then not showing the boxes. Sadly this is the easiest way to do this in python\n    \n\"\"\"\np = sns.stripplot(data=df, x=\"hair_color\", y=\"pain_threshold\")\n\n\nsns.boxplot(\n    data=df,\n    x=\"hair_color\",\n    y=\"pain_threshold\",\n    showmeans=True,\n    meanline=True,\n    meanprops={\"color\": \"k\", \"ls\": \"-\", \"lw\": 2},\n    medianprops={\"visible\": False},\n    whiskerprops={\"visible\": False},\n    zorder=10,\n    showfliers=False,\n    showbox=False,\n    showcaps=False,\n    ax=p,\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.violinplot(data=df, x=\"hair_color\", y=\"pain_threshold\")\n\n\n\n\n\n\n\n\n\npg.normality(df, group=\"hair_color\", dv=\"pain_threshold\")\n\n\n\n\n\n\n\n\nW\npval\nnormal\n\n\nhair_color\n\n\n\n\n\n\n\nLight Blond\n0.991032\n0.983181\nTrue\n\n\nDark Blond\n0.939790\n0.664457\nTrue\n\n\nLight Brunette\n0.930607\n0.597974\nTrue\n\n\nDark Brunette\n0.883214\n0.324129\nTrue\n\n\n\n\n\n\n\nTests of normality Caveats:\nKeep in mind that when the sample size is small, the test for normality may not have enough power to make an accurate prediction. This is true for all the parametric tests we do.\nAlso, very few, if any, real world data are perfectly normal. Most of the time we are trying to predict the mean, and not the extremes, and so it doesn’t really matter, because in large samples the means will be normally distributed (central limit theorum).\nNon Parametric tests are robust to outliers and large sample sizes, and are a good alternative when there is ambiguity about the normality of the data.\n\nimport scipy.stats as stats\n\nstats.f_oneway(\n    df[\"pain_threshold\"][df[\"hair_color\"] == \"Light Blond\"],\n    df[\"pain_threshold\"][df[\"hair_color\"] == \"Light Brunette\"],\n    df[\"pain_threshold\"][df[\"hair_color\"] == \"Dark Brunette\"],\n    df[\"pain_threshold\"][df[\"hair_color\"] == \"Dark Blond\"],\n)\n\n# I prefer the pingouin output because it gives more information\n\nF_onewayResult(statistic=6.791407046264094, pvalue=0.00411422733307741)\n\n\n\naov = pg.anova(dv=\"pain_threshold\", between=\"hair_color\", data=df, detailed=True)\naov\n\n\n\n\n\n\n\n\nSource\nSS\nDF\nMS\nF\np-unc\nnp2\n\n\n\n\n0\nhair_color\n1360.726316\n3\n453.575439\n6.791407\n0.004114\n0.575962\n\n\n1\nWithin\n1001.800000\n15\n66.786667\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nThe hair color has a p &gt; alpha, so we reject the null hypothesis. Hair color has been shown to have an effect.\nThe eta squared (np2) indicates that 57.59% of the variance in pain thresholds can be explained by the hair color!!\nIf there were more than one grouping variable, say, they also collected the gender and if the person was right handed or left handed, this could also be computed using an anova. And if there were other factors that could affect pain threshold, we would test them using an ancova\nThe number of groups helps us name the test. In the present experiment, we have done an 1-way anova, because there is one grouping variable. If there were 4 grouping variables, it would be a 4-way anova\nCalculating the power of the study\n\nnum_groups = df[\"hair_color\"].nunique()\n\nob_per_group = df.shape[0] / num_groups  # Mean number of observations per group\n\n\nachieved_power = pg.power_anova(\n    eta_squared=aov.loc[0, \"np2\"], k=num_groups, n=ob_per_group, alpha=0.05\n)\nprint(\"Achieved power: %.4f\" % achieved_power)\n\n# you can also look up the F Stat significance in tables available online\n\nAchieved power: 0.9729\n\n\nCalculating the power of the study if this were particle physics, where the alpha is much much smaller\n\nnum_groups = df[\"hair_color\"].nunique()\nob_per_group = df.shape[0] / num_groups  # Mean number of observations per group\n\n\nachieved_power = pg.power_anova(\n    eta_squared=aov.loc[0, \"np2\"], k=num_groups, n=ob_per_group, alpha=0.00000057\n)\nprint(\"Achieved power: %.4f\" % achieved_power)\n\n# If you fiddle around with the alpha, you would see how much the power varies\n\nAchieved power: 0.0089\n\n\nRead more about the selection of alpha and its effects and how to pick a good alpha here: Miller, J., & Ulrich, R. (2019). The quest for an optimal alpha. PloS one, 14(1), e0208631.\n\n\nAncova\nType: Parametric\nApplied to:\n\nDependant Variable = continuous\nIndependent Variable = categorical with 2 or more categories (nominal or ordinal)\nCoVariates = continuous\n\nRationale:\nIn many studies we are trying to make changes to or predict the changes to a dependant variable (DV) using an independent variable (IV). eg. Weight is being modified by total calorie intake or test scores being modified by the type of teaching method. In these situations, there could be other variables that also have an effect on the dependant variable. eg. In case of Weight, maybe the BMR of the patient matters. In case of the test scores maybe poverty matters. These are called the covariates (CV) of the DV. These contribute to the outcome heterogeneity.\nIf we want to know the precise effect of the intervention (IV) like calorie restriction, and we do not account for all the things that make a difference to the dependant variable, we will have lower power and a less accurate result. Measuring the covariance maximizes power and precision.\nSo to know how much of the change (variance) in the DV is being caused by these covariates, we need to break the overall variance down and separate out the variance caused by the covariates from the variance caused by the IV.\nIf the covariates are continuous variables, the test we use is Ancova, which is an ANOVA+regression. Ancova also gives us the statistical power of the test.\nANCOVA decomposes the variance in the DV into\n\nVariance explained by the covariates,\nVariance explained by the categorical independent variable, and\nResidual variance.\n\nOverall, what we are trying to do is answer if patient A got treatment X instead of treatment Y, by how can we expect them to differ in outcomes. This effect is composed of the effect of the treatment, effect of covariates and residuals (unknowns).\nNull Hypothesis:\nh_0 : There is no relationship between the IV and the DV, controlling for the covariates\nAssumptions:\n\nA normal distribution of the covariates and dependant variable values in the sample.\nEach category/group that is being looked at should have at least 30 cases.\nThe covariate and the treatment are independent (In experiemental studies)\nAt each level of categorical independent variable, the covariate should be linearly related to the dependent variable (if the relationship is non linear, it reduces the power of the study)\nEquality of variances (Homoscedasticity)\n\nRead More:\n\nBiostats for Biomedical Research : Ancova\nStats and R: Anova in R\nWhen Assumptions of ANCOVA are Irrelevant\nANCOVA Assumptions: When Slopes are Unequal\n\nDataset: We are using Pingouin’s inbuilt ancova dataset. In this study, they randomized students into different teaching methods and measured the scores. They also measured the BMI of the students and the Income. Our hypothesis here is the method has a significant effect on the scores, but we also want to know if other things like income and BMI have any role.\n\nimport numpy as np\nimport pandas as pd\nimport pingouin as pg\n\n\ndf = pg.read_dataset(\"ancova\")\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 36 entries, 0 to 35\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Scores  36 non-null     int64  \n 1   Income  36 non-null     float64\n 2   BMI     36 non-null     int64  \n 3   Method  36 non-null     object \ndtypes: float64(1), int64(2), object(1)\nmemory usage: 1.3+ KB\n\n\n\nsns.pairplot(df, diag_kind=\"hist\")\n\n\n\n\n\n\n\n\nWe can see that there does seem to be a linear relationship between the Income and scores, where as income and BMI do not appear correlated. The data do not appear to be normally distributed, but we can do more formal tests of this to be sure.\n\n# Assumption testing:  Normality\n\n\nimport scipy.stats as stats\n\nprint(\"Income: \", stats.normaltest(df.Income))\nprint(\"BMI: \", stats.normaltest(df.BMI))\nprint(\"Scores: \", stats.normaltest(df.Scores))\n\n# p &gt; alpha (0.05), accept the null hypothesis. The distribution of variables is normal.\n\nIncome:  NormaltestResult(statistic=1.8694971178536874, pvalue=0.392684590154046)\nBMI:  NormaltestResult(statistic=5.135141170153041, pvalue=0.0767217080617244)\nScores:  NormaltestResult(statistic=3.517920062610699, pvalue=0.172223878164023)\n\n\n\nimport seaborn as sns\n\nax = sns.countplot(x=df.Method)\n\n# Unequal sizes of each treatment group\n\n\n\n\n\n\n\n\n\n# Assumption testing: Equal variance\n\nimport matplotlib.pyplot as plt\n\np = sns.stripplot(\n    x=df.Method, y=df.BMI, dodge=True, size=8, marker=\"D\", alpha=0.5, hue=df.Method\n)\n\nsns.boxplot(  # This draws a line at the mean of each group\n    showmeans=True,\n    meanline=True,\n    meanprops={\"color\": \"k\", \"ls\": \"-\", \"lw\": 2},\n    medianprops={\"visible\": False},\n    whiskerprops={\"visible\": False},\n    zorder=10,\n    x=\"Method\",\n    y=\"BMI\",\n    data=df,\n    showfliers=False,\n    showbox=False,\n    showcaps=False,\n    ax=p,\n)\nplt.show()\n\n\n\n\n\n\n\n\nThe groups seem to have roughly similar variance. Approximately the same number of values above and below the mean marker\n\npg.ancova(data=df, dv=\"Scores\", covar=[\"Income\", \"BMI\"], between=\"Method\")\n\n\n\n\n\n\n\n\nSource\nSS\nDF\nF\np-unc\nnp2\n\n\n\n\n0\nMethod\n552.284043\n3\n3.232550\n0.036113\n0.244288\n\n\n1\nIncome\n1573.952434\n1\n27.637304\n0.000011\n0.479504\n\n\n2\nBMI\n60.013656\n1\n1.053790\n0.312842\n0.033934\n\n\n3\nResidual\n1708.508657\n30\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nSS is the sum of squares\nDf is the degrees of freedome\nF is the F statistic or the parameter being estimated\nP-unc is the uncorrected p value\nnp2 is the partial eta squared or the effect size\n\nIn the above example, the effect size of the method explains 24.42% of the variance in the scores, and income explains 47.95%, both have significant p values. and BMI does not have a significant p value and so doesn’t play a role.\nThis means that the effect size of method of teaching is only about half of the effect size of the income.\nAbout 27% of the remaining effect is unexplained. so there are probably other factors that we have not measured or theorized about that are causing this heterogeneity.\nNote that we have not answered the question which type of method of teaching is the best. This is done using an anova if there are more than 2 groups, and a student’s t-test if there are two.\nThere is no non-parametric test that adjusts for covariates"
  },
  {
    "objectID": "blog/biostat-with-python-01/01-BWP-deploy-bold.html#other-tests",
    "href": "blog/biostat-with-python-01/01-BWP-deploy-bold.html#other-tests",
    "title": "Statistical tests for biomedical research with python",
    "section": "Other tests",
    "text": "Other tests\n\nShapiro-wilk\n\nThis is a test of normality of data\nwhere non parametric tests are not useful or to understand the data better.\nThe statistic by itself cannot be interpreted, the p-value is calculated using montecarlo and interpretation is based on that.\n\nThe null-hypothesis of this test is that the population is normally distributed.\nSo if the p value is less than the chosen alpha level, then there is evidence that the data are not normally distributed.\nThis is useful when the sample n is &lt; 5000\nCaveats:\n\nWhen the sample size is small, the test may not have enough power to make an accurate prediction. So you get false negative that the data is normal\nWhen the data is large, even a few large outliers can cause a false positive that the data is not normal.\nVery few, if any, real world data are perfectly normal. linear models generally involve the assumption that their residuals i.e., the differences between observed and predicted values are normally distribted.\nMost of the time we are trying to predict the mean, and not the extremes, and so it doesn’t really matter, because in large samples the means will be normally distributed(central limit theorum).\nNon Parametric tests are robust to outliers and large sample sizes, and are a good alternative when there is ambiguity about the normality of the data.\n\nRead More: 1. Shatz, I. Assumption-checking rather than (just) testing: The importance of visualization and effect size in statistical diagnostics. Behav Res 56, 826–845 (2024). https://doi.org/10.3758/s13428-023-02072-x\n\nimport pandas as pd\nfrom scipy.stats import shapiro\n\ndf = pd.read_csv(\"mann-whitney-eva-data.csv\")\n\n\nshapiro(df[\"BDI_Sunday\"])\n\n# Since the p is lesser than the alpha, the data are not normally distributed\n\nShapiroResult(statistic=0.7481864947430416, pvalue=0.000160534418300268)\n\n\n\ndf_t = pd.read_csv(\"paired_t_bp.csv\")\n\n\ndf_t.head()\n\n\n\n\n\n\n\n\nbase_bp\npost_drg_bp\n\n\n\n\n0\n125.4\n98.3\n\n\n1\n118.9\n116.8\n\n\n2\n93.5\n96.0\n\n\n3\n131.2\n114.4\n\n\n4\n120.7\n118.7\n\n\n\n\n\n\n\n\nstats.shapiro(df_t[\"base_bp\"][:500])\n\n# p &gt; alpha, reject null hypothesis. sample is normal.\n\nShapiroResult(statistic=0.9954274847935575, pvalue=0.15032071619683773)"
  },
  {
    "objectID": "blog/sem-search-notes/index.html",
    "href": "blog/sem-search-notes/index.html",
    "title": "Bold: Research and Development",
    "section": "",
    "text": "“If cats looked like frogs we’d realize what nasty, cruel little bastards they are. Style. That’s what people remember.” ― Terry Pratchett, Lords and Ladies"
  },
  {
    "objectID": "blog/sem-search-notes/index.html#steps-for-implementing-semantic-search-on-a-text-corpus",
    "href": "blog/sem-search-notes/index.html#steps-for-implementing-semantic-search-on-a-text-corpus",
    "title": "Bold: Research and Development",
    "section": "Steps for implementing semantic search on a text corpus",
    "text": "Steps for implementing semantic search on a text corpus\n\nClean and process corpus\n[optional] Enrich the data so that there are features in the data that assist better matches to queries.\nEmbed corpus on a sentence embedding model\nIndex embedding using some ANN like NGT/FAISS/HNSW\nEmbed query using the same model\nSearch index using query embedding\n[optional] Rerank the hits using a cross encoder\nServe the top k hits"
  },
  {
    "objectID": "blog/sem-search-notes/index.html#mechanism-of-semantic-search",
    "href": "blog/sem-search-notes/index.html#mechanism-of-semantic-search",
    "title": "Bold: Research and Development",
    "section": "Mechanism of semantic search",
    "text": "Mechanism of semantic search\nMasked language modelling used specifically to create sentence embeddings creates a representation of each sentence in corpus with an n-dimensional relationship with every other sentence in the corpus. So instead of isolated words, meanings of phrases and paragraphs are encoded. Depending on the training and model, these relationships could be accessed using cosine similarity or euclidean distances.\nWhen we encode the query using the same model, we make the query exist in the same vector space as the rest of the corpus. This means that the distance metric used for encoding can be used for decoding.\nSearch is calculating the cosine-distance between the query and the text in the corpus and returning the top ranked or closest results. This means the poems that get recommended contain terms or phrases or words that are similar to the query. This allows for approximate meaning matching. Instead of searching seperately for knife|fork|spoon , just search for cutlery and all the related text will be found.\nIf you are searching for highly deterministic answers in a document corpus, let’s say a medical data corpus and you want to find out the different antibiotics used for disease x, this is very useful. This is a pure retrieval kind of task and pure semantic retrieval is a great addition to, or improvement on pure lexical search. I explore the differences in my post here: What is semantic search and wny do we want it (https://anandphilip.com/what-is-semantic-search-and-why-do-we-want-it/)."
  },
  {
    "objectID": "blog/sem-search-notes/index.html#training-notes.",
    "href": "blog/sem-search-notes/index.html#training-notes.",
    "title": "Bold: Research and Development",
    "section": "Training Notes.",
    "text": "Training Notes.\nWhen it comes to poetry, users desire recommendations, not just matches. Instead of finding poems that merely contain the query’s terms or related words, we seek instances where the themes, emotions, or concepts in the query are explored within the poems. This is a nuanced challenge.\nIf I search for “our relationship with technology” instead of matching poems that contain th term or related terms to “relationship” and or “technology”, I want instances of the relationship explored in the poems.\nFeatures that are relevant to the recommendations are things like\n\nSettings - physical setting like sunrise, sunset dusk etc. emotional settings like break ups, winning a lottery, death etc.\nThemes as in the emotive theme or philosophical theme , eg dealing with death, impermanence of life, nature\nTone- humor, sarcasm, bitterness\nStyle - acrostic, free verse, etc.\n\nIn order to improve the retrieval, I used VLLM and Wizard Vicuna to enrich the documents, I generated a list of emotions, themes and tones of the poem which I appended to the poem before embedding so that some descriptive stuff is also added to the retrieval. I think this has improved the retrieval a lot. but still, it’s tricky to coax a model into coherently give you answers to “this poem is an instance of” to enrich the document.\nWhen I was first reading about semantic search, I thought this would be search that understood my query and then found what was matching that. But really what search of any kind does (as long as the unit of query is words and fitness of match is also based on words) is optimizing the similarity between the query and the text. This to me is just a variation of lexical search. So embedding based retrieval is better at lexical semantics than really parsing the query and extracting the meaning from it.\nBy lexical semantics here I am referring to the structural and functional relationships between word and their meanings that are encoded in text corpora. IN linguistics these are represented as semantic networks or ontologies. The word color is a semantic neighbor of the words red, green and yellow. confusingly they are not lexical neighbors.\nWhat would be cool is if you could search for ‘bitter poem about love set in a cafe’. in this case if we had ways to identify the different components of the query and then run separate searches, or some chaining of searches, that could improve things. eg.\n\nsearch tone : bitter\nsearch theme: love | loss\nsearch setting: cafe\n\nBut as you can imagine, this creates too many moving parts to manage.\nA shortcut would be use a LLM to do query expansion and that is the approach most current papers and large organizations seem to be exploring. It’s just that that is so resource intensive, and generally needs cloud computing.\nCould this situation be improved using different types of embeddings. eg. embeddings that are designed for topic modelling or identifying and retaining topic and ontological models of the text? I think so. But do not have enough empirical evidence.\nAs a thinking tool, semantic search is a definite improvement over lexical but I think there are still many problems left to solve here."
  },
  {
    "objectID": "blog/sem-search-notes/index.html#recommendations-not-search",
    "href": "blog/sem-search-notes/index.html#recommendations-not-search",
    "title": "Bold: Research and Development",
    "section": "Recommendations, not search",
    "text": "Recommendations, not search\nThe difference between search and recommendation, as in the engineering of these things, is that search is a function that takes in the query (q) and matches it to the corpus (c) to produce a result, where as recommendations add a variable – the user’s known or presumed preferences (p) to the same function (or even without a query, just the \\(p\\) and the \\(c\\)).\n\\(search\\_result = f(q, c)\\)\nwhereas\n\\(recommendation = f(q, p, c)\\)\nThe \\(p\\) can be derived in a wide variety of ways. The most popular one is collaborative filtering, in which if the user says i like this poem, you then look at all the other people who like this poem, find the other poems they liked, and recommend them to this user. This assumes that people who like things like similar things.\nOther components of \\(p\\) could be\n\nThe item’s properties (long poem, short poem, language etc.)\nThe user’s preferences and interaction history (liked 3 poems by Plath and is now searching for microwaves)\nA query or other information need provided by the user (in an imaginary interface, the user could point to a poem and say, similar but less sad)\nThe context at the point and time of recommendation\nBusiness logic (show sponsored results first for eg.)\n\nThis is explored more in Michael D. Ekstrand and Joseph A. Konstan. 2019. Recommender Systems Notation: Proposed Common Notation for Teaching and Research. Computer Science Faculty Publications and Presentations 177. Boise State University. DOI 10.18122/cs_facpubs/177/boisestate. arXiv:1902.01348"
  },
  {
    "objectID": "blog/sem-search-notes/index.html#converting-search-into-recommendations",
    "href": "blog/sem-search-notes/index.html#converting-search-into-recommendations",
    "title": "Bold: Research and Development",
    "section": "Converting search into recommendations",
    "text": "Converting search into recommendations\n\nAdd some way of taking in user preferences either upfront, or for each search result\nUse these to create (or match) a features for your corpus\nupdate the search and ranking method to add these features\n\nMore information about design patterns in recommendation systems can be found here: Yan, Ziyou. (Jun 2021). System Design for Recommendations and Search. eugeneyan.com."
  },
  {
    "objectID": "blog/opinionated-RAG-heuristics/index.html",
    "href": "blog/opinionated-RAG-heuristics/index.html",
    "title": "Four Opinionated Heuristics For RAG in Medicine",
    "section": "",
    "text": "She’d been a kitchen maid and now she was subjecting the Book to critical analysis and talking to a religious icon. That sort of thing led to friction. The presence of those seeking the truth is infinitely to be preferred to the presence of those who think they’ve found it. – Polly Perks, Monstrous Regiment\nThe stakes are (relatively) low when\nThe most touted uses of RAG in medicine are question answering and paper or information summarization.\nThe state of the art in medical question answering on closed, carefully selected datasets using models that in all likelihood have had access to the test data, is around 80% which means the probability of getting at least one wrong answer in 10 is around 89%.The median numbers are a whole lot worse. If you can’t afford the latest model to be answering your questions or doing the generation, imagine the possibility of error. (or do the math, your call)\nStats for summarization are a whole lot worse. And a whole lot tougher to quantify. But, if you want some quick summaries, with the aim of deciding what to focus on, or what to read etc. this is probably pretty good.\nAnother low stakes situation is when the data is small. Summarizing a couple of paragraph is likely to have less errors than summarizing a paper. or summarizing multiple papers at once. Context lengths are not created equally.\nRAG that is generation heavy is going to be about as good as vanilla generation, and this excels at boilerplate generation, ideation, rephrasing the immensely common, responds to carefully worded/structured prompts, has high accuracy with chunked generation, and is great for iterative interactions.\nThe evidence for the awesomness of graph embeddings and knowledge graphs in the LM contexts are not all that clear, but they have been shown to reduce halluciations and that is good enough for me.\nThe above heuristics are for deployment in the wild, not personal or research purposes."
  },
  {
    "objectID": "blog/opinionated-RAG-heuristics/index.html#footnotes",
    "href": "blog/opinionated-RAG-heuristics/index.html#footnotes",
    "title": "Four Opinionated Heuristics For RAG in Medicine",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nChain of thought prompting is probably useless, can justify things in an authoritative sounding way, which can confuse and waylay the user.↩︎\nI know, circular reasoning, but low stakes are easier to identify than define.↩︎"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Bold: Research and Development",
    "section": "",
    "text": "BSD 2-Clause License\nCopyright (c) 2024, Anand Philip\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Anand Philip",
    "section": "",
    "text": "I’m a family physician interested in how information technology can improve (or worsen) decision making in and outside healthcare.\n\nThis fuels Bold, my health-tech R&D practice where I build tools for my clients using the best technology for the problem. Sometimes these are ML/AI models and sometimes these are guidelines.\nRead more about specific services I offer here"
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Hi, I’m Anand Philip",
    "section": "Updates",
    "text": "Updates\n\n\n\nEthics \n\nI have put togeter a basic reading list and explanation of the main schools of thoughts about Ethics in AI [external link]\nI am editing an Anthill Inside session on AI and Risk Mitigation. My co-editor Dr. Hannah Thomas and I are working with Health-AI companies to develop priorities, guidelines and resources for the use of AI in healthcare in India.\n\nAI Challenges in Radiology A discussion with 5C Network\nEthical Concerns in Use of AI in Healthcare A discussion with Dr. Olinda Timms\n\n\n\n\nSemantic Search \n\nLast year I did a deep dive into semantic and vector based search, made a poetry recommendation app and generally had a bunch of fun learning a lot of new stuff. If you’ve ever wanted better search, semantic search is one of the solutions. This is an area I am actively working on.\n\nWhat is semantic search and why do we want it. This is a non-technical explanation.\n\nTechnical notes about semantic search and how to implement it\nThe repo has training notebooks and more\n\n\n\n\nStatistics \n\n\nIn collaboration with Medibuddy, I created a guide to conducting and selecting statistical tests using Python. This guide serves as a quick reference for their research team, promoting standardization and efficiency in research workflows. You can read the whole guide and run it on colab here:\n\nParametric and non-parametric tests in  python\nWatch the repo for further notebooks/installations as this is under active development\n\n\n\n\nLatest on the Blog\n\n\nFour heuristics for RAG in Medicine"
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "Hi, I’m Anand Philip",
    "section": "Get in touch    ",
    "text": "Get in touch"
  },
  {
    "objectID": "index.html#fuelled-by",
    "href": "index.html#fuelled-by",
    "title": "Hi, I’m Anand Philip",
    "section": "Fuelled by:      ",
    "text": "Fuelled by:"
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Services offered at BOLD",
    "section": "",
    "text": "I provide consulting services to startups seeking insights and guidance from a clinical technologist specializing in data/ML/AI and decision-related domains. I am as happy to talk strategy as doing data modelling. Especially if we can talk about ethics and risk upfront.\nContact me if you would like to work on applied machine learning /AI in healthcare or really anything ML. Email, Mastodon, Linkedin are all good ways to get in touch. Details below.\nI am mostly located in Bengaluru, India."
  },
  {
    "objectID": "services.html#get-in-touch",
    "href": "services.html#get-in-touch",
    "title": "Services offered at BOLD",
    "section": "Get in touch    ",
    "text": "Get in touch"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Bold Research Blog",
    "section": "",
    "text": "Four Opinionated Heuristics For RAG in Medicine\n\n\n\n\n\n\n\n\n\nNov 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResources I used for making this quarto website\n\n\n\n\n\n\n\n\n\nMay 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical tests for biomedical research with python\n\n\nA guide to conducting and selecting statistical tests in Python\n\n\n\n\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is semantic search and why do we want it\n\n\nHow does search work\n\n\n\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSteps for implementing semantic search on a text corpus\n\n\ntraining notes\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/resources-for-making-quarto-blog/index.html",
    "href": "blog/resources-for-making-quarto-blog/index.html",
    "title": "Resources I used for making this quarto website",
    "section": "",
    "text": "“The trouble with having an open mind, of course, is that people will insist on coming along and trying to put things in it.” ― Terry Pratchett, Diggers\nAs with any new project, one of the best ways to (productively) procrastinate is to decide to learn some new tech. I’ve been putting up wordpress websites since the beginning of time and I am kinda bored of it. Most of my writing is in python, markdown files and jupyter notebooks, and so i figured a markdown and code friendly publishing platform would be a good thing. I looked around at the billion static site generators on Jamstack’s list, picked the ones that were python friendly and tried out Pelican, Lektor, Nikola, Hugo, Gatsby and skipped Jekyll because I had tried it out months ago.\nAll the tested SSGs were fast and easy enough to put up a basic website. But boy when it came to customizing the theme, adding special features and even something as simple as search, the experience was not fun at all.\nI finally settled on Quarto because\nAlong the way, I discovered several useful tools and blogs that were of great help and here they are.\nOverall, I probably spent the most time on Quarto’s documentation, which, while sparse, are very good for setting things up and learning the basics.\nMarvin Schmitt’s template and detailed posts about how go about doing everything from setting up a site and structuring it were the most help inititally\nI ended up with a refresher course on CSS’s grid system, and overall for boostrap the best guide was their website. A huge number of seemingly bot generated websites exist for bootstrap theme customization, but boy they suck.\nElla Kaye’s website was a huge help. I’ve learned a lot from her config files on the github repo\nSilvia Canelon’s detailed documentation of how she ported from hugo was super useful in understanding the quirks of the system\nSam Csik’s guide on how to set up a blog is what I have used to set up this blog\nI admit I spent an inordinate amount of time on this palette generator\nI used the Huemint bootstrap theme generator to generate the css/scss for colors\nFinally, Susie Lu’s Viz Palette app helped me with figuring out what color combos were best for readability and for those with visual problems.\nPS: I went down a rabbithole of designing perceptually uniform color palettes but then I realized, many moons later that while we need to use these for designing visualizations, a website’s color design doesn’t need to be perceptually uniform."
  },
  {
    "objectID": "blog/resources-for-making-quarto-blog/index.html#footnotes",
    "href": "blog/resources-for-making-quarto-blog/index.html#footnotes",
    "title": "Resources I used for making this quarto website",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI am a data scientist, which is not a scientist at all, but an engineer of sorts. The confusion continues.↩︎"
  },
  {
    "objectID": "blog/what-is-semantic-search/index.html",
    "href": "blog/what-is-semantic-search/index.html",
    "title": "What is semantic search and why do we want it",
    "section": "",
    "text": "“If cats looked like frogs we’d realize what nasty, cruel little bastards they are. Style. That’s what people remember.” ― Terry Pratchett, Lords and Ladies\nThis post originally appeared on: anandphilip.com\nYou’re searching for poems that talk about cutlery. Because you can. And also because compiling a list of poems about our relationship with food-ingestion-tools would be fun.\nMaybe it’s not poetry you care about but the name of the company your boyfriend works at which he told you about three months ago but cant find on your Whatsapp. And obviously, it would be super awkward to ask him now.\nWe have disparate search needs is what I am trying to say."
  },
  {
    "objectID": "blog/what-is-semantic-search/index.html#lexical-search-how-most-search-works",
    "href": "blog/what-is-semantic-search/index.html#lexical-search-how-most-search-works",
    "title": "What is semantic search and why do we want it",
    "section": "Lexical search: how most search works",
    "text": "Lexical search: how most search works\n\n\n\nDefinitions\n\n\nQuery: The text we enter into search fields when we want stuff found\nDocument: The file, website, chat, image, meme stash or entity that has the terms or could have the the right answers to your search\nCorpus: The collection of  all the documents that need to be searched across various searches. (your whole website for example)\n\n\n\nThe standard way in which we do search is by matching the words or terms in a query with the terms in the corpus. So if you search for poems about cutlery  the search will look for documents that have the words poems and/or cutlery and try to find ones that contain both the words.\nImagine, though, that the program has to scan through every document every time someone searches for something, hunting for instances of the word from scratch. This would be pretty time-consuming and also slightly stupid. So we need a way to store what words are in what documents.\nBesides just matching or finding the word, we also need some way of knowing which document matches it better. This is called ranking. To solve both these problems  people came up with the idea of indexes (or indices, if you’re a Latin snob).\nWhat an index does is, it look over all the documents in a corpus, and creates a brief representation of the content of that document. So instead of searching over the whole document, now you just search over the representations. These representations also have some way of ranking the quality of the fit between the query and the document.\nAn example of such a representation1 is Term Frequency–Inverse Document Frequency (TF-IDF). What this does is,\n\nLists all the words in each of the documents in a corpus and how frequently the occur in a document (term frequency).\nCalculates how common or how rare of each of these words are across the whole corpus. The common words get a score of 0, or close to zero and the rare words get high scores. 2\n\nWhy do we do this? In most documents, the words that appear most frequently would be things like the, and, is and stuff that doesn’t add any discriminatory power to the retrieval.\nWhen I search for poems with cutlery  there are going to be a whole lotta  poems with the word with in them, so I can’t use with to rank the documents in any way. But the word cutlery probably appears in a small number of poems, so it is a good word for ranking the results.\n\nSo now your index of poems doesn’t just have the whole text of the poem but a list of the non-zero scoring words along with the scores, which allows us to get the top matches.\nThe most used and probably the best of these TF-IDF  algorithms is the  Okapi BM25  family of algorithms. They are absolutely fantastic in finding you the documents that are best matching the words you search for.\nBut what if you don’t know the precise thing you are searching for? This is also a problem as old as misplaced keys. One way to solve this would be to have some kind of a thesaurus. You search for cutlery  and we expand the terms to include all the synonyms of the word. This is called query expansion. So instead of sending the term cutlery to the index, we can send cutlery, knife, fork, teaspoon and see what the index gives us.\nBut that means now you need to figure out which words you want to find synonyms for, how to store them, how to update them periodically etc. And what if you need to find synonyms of phrases, not words, eg. our relationship with modern day digital technology. That would make very large search term if we expanded each word. Still, this is a pretty damn good method and is used widely and is what Google uses to return close matches.\n\nAnother problem could be that you might misspell the word, or that your way of spelling antidisestablishmentarianism3 is different from the way it appears in the text, say “anti-dis-establishmentarianizm” or whatever.  The usual solution to this is to add some “fuzzy” logic or approximate matching. This works by searching for words that are off or different by a few letters. Unfortunately this means that if you search for costly coat cuts,  you’ll also get results for costly cat guts 4.  Still, this too is pretty damn good and is usually  a part of most search systems.\nYou could also offer spelling correction, as google often does. But problems with autocorrect are well known\n\nI am compelled to point to yet another problem: Besides synonyms, there are words that are structurally similar, or stem from the same word. These too might make a good fit or a bad fit for a particular search query based on the context, for eg. if you’re searching for load bearing walls, it might be good to get results for load bearing tiles, but not good to get bear loading 5\n\nAnyway, so what we need is a way to represent text that can account for\n\nVariations in the way a word is written\nContextual meaning of words (A coat of paint vs a coat of mink)\nSpelling mistakes\nSynonyms\n\nAll these things are present in the way we write and store information. So if you were to take a sufficiently large sample of things people have written, you would find all of these things in this sample. This idea is what gives us semantic search."
  },
  {
    "objectID": "blog/what-is-semantic-search/index.html#enter-ai",
    "href": "blog/what-is-semantic-search/index.html#enter-ai",
    "title": "What is semantic search and why do we want it",
    "section": "Enter AI!",
    "text": "Enter AI!\n\n\n\nCreated with Dall-E\n\n\nKidding, AI is a marketing myth, who wants to buy “statistical-learning models”?\nBut that is what we do, we take a large corpus of text, and then derive a statistical model of the relationships between words in a multi dimensional space. And we do this by masking text and getting a model to fill in the blanks. No, seriously, that’s it. This is the basis of all LLMs.\nWhat this does is, make a model memorize and therefore extract all the million ways in which any given word can be used in a language and also how each word in the corpus is related to every other word. This is not something we can manually compute and that is why we throw artificial neural networks at it.\nNow, most LLMs and other things that are called AI these days don’t stop here, and have to predict specific things. But if you take off those prediction bits from the model what you have left is called an embedding. Which is a multi-dimensional representation of whatever it is that you trained the thing to do. Engineers, like many of us, suffering from grievous mathematics envy, call these representations vectors.\nThe way these work is that words and documents that are similar in meaning are represented closer to each other, or clustered. But this happens over many dimensions, so it’s not a simple 1 or 2 D relationship it captures.\n\nSee how the word noticed is close to, and connected to all the words that are similar in meaning. The above is a screenshot of one such embedding. I strongly urge you to explore this. Link: Word2Vec Tensorboard\nAnother tool to understand how words can be represented in multiple dimensions is this embedding explorer from Edwin Chen.\nWhat is most interesting to me is that this rich and pretty accurate representation is produced using something as simple as fill-in-the-blanks."
  },
  {
    "objectID": "blog/what-is-semantic-search/index.html#semantic-search",
    "href": "blog/what-is-semantic-search/index.html#semantic-search",
    "title": "What is semantic search and why do we want it",
    "section": "Semantic Search",
    "text": "Semantic Search\n\nSearching for things using the information implicitly encoded in these vector spaces is called semantic search by software and ML people.6\n\nSo if you don’t want to manually manage everything from synonyms to contextual meaning, you could take a vector space trained on a very large corpus, convert your corpus into those kind of vectors, and then search over those vectors. This conversion just borrows the relationships that are already discovered by the model, which means its representations will be much richer than the ones in your corpus.7\nThese vector spaces are not like the indexes we spoke of earlier, they don’t have any explicit way of saying word x is present in document z and the score is high.\nThe search works by identifying documents that are “close” to the query in the vector space. The most commonly used metric to determine this distance is cosine similarity. But even if you can calculate the cosine distance between two words, doing this calculation for every word in your corpus against every word in the query would be mind-numbingly boring and also time and resource consuming. So we need some way to find the location in the vector space that is most likely to have good results, and get there quickly.\nThis is called approximate nearest neighbor search, and there are a bunch of great algorithms that have come up in the last few years that let you search over humongous datasets really fast.\nThe ones I like the most are Neighborhood Graph Trees and Hierarchical Navigable Small Worlds (HNSW)\nBoth of them work by dividing up your data into small clusters, or trees and then searching in those trees or clusters. But really, it’s a lot more complicated than that and to be honest I don’t really understand graph theory enough to know what they do. But what they do they do do well.\nThis might remind you of indexes, and that is what these create, just, vector and tree representations, not words.\n\nimage from Pinecone’s guide to semantic search, linked below"
  },
  {
    "objectID": "blog/what-is-semantic-search/index.html#summary",
    "href": "blog/what-is-semantic-search/index.html#summary",
    "title": "What is semantic search and why do we want it",
    "section": "Summary",
    "text": "Summary\n\nYou embed your corpus onto an existing language model’s vector space\nYou then index your embedding using a ANN algorithm like NGT or HNSW\nAnd then you can search your corpus using semantic querying.\n\nThanks for reading.\n\nThis cat does not exist. Created with DeepAI"
  },
  {
    "objectID": "blog/what-is-semantic-search/index.html#citation",
    "href": "blog/what-is-semantic-search/index.html#citation",
    "title": "What is semantic search and why do we want it",
    "section": "Citation",
    "text": "Citation\n\nIf you wish to cite this page, here you go\nPhilip. (2023, November 1). What is semantic search and why do we want it. Anand Philip’s Blog. Retrieved February 22, 2024, from https://anandphilip.com/what-is-semantic-search-and-why-do-we-want-it/"
  },
  {
    "objectID": "blog/what-is-semantic-search/index.html#further-reading-and-resources",
    "href": "blog/what-is-semantic-search/index.html#further-reading-and-resources",
    "title": "What is semantic search and why do we want it",
    "section": "Further reading and resources",
    "text": "Further reading and resources\n\nViki Boykis has a great resource page and explanation of what embeddings are\nANN-Benchmarks: This website has benchmarks for every approximate nearest neighbor algorithm you can think of [NGT and HNSW are the best]\nPinecone, which is a vector database solution, has a great series on Natural Language Processing for Semantic Search"
  },
  {
    "objectID": "blog/what-is-semantic-search/index.html#footnotes",
    "href": "blog/what-is-semantic-search/index.html#footnotes",
    "title": "What is semantic search and why do we want it",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is not the most commonly used representation, sometimes you just store the title of the document, somethings you store a description, it depends on the need of the system and users.↩︎\nEngineers really love logarithms. We should too↩︎\nI am, of course a disestablishmentarianist and several centuries behind on news↩︎\nIf you’re into coat cuts. Whatever they are, no judgement here.↩︎\nI know, my metaphors need work. They are unemployed↩︎\nComplicating the matters is that before there were large neural network based embeddings, there were the semantic web people and the ontology people and other people who wanted to solve this using structured representation of text. So software people are kinda stealing the term a little bit. More info: A survey in semantic search technologies↩︎\nThankfully there is a huge community of NLP nerds who like making these kinds of models easy to use (if you can Python, that is).↩︎"
  }
]